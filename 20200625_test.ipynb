{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width      species\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/iris.csv', names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]) \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "dataset\n",
    "X = dataset[:,0:4]\n",
    "Y = dataset[:,4]\n",
    "X=np.asanyarray(X).astype(np.float32)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "e = LabelEncoder()\n",
    "e.fit(Y)\n",
    "Y = e.transform(Y)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y = tf.keras.utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 학습셋과 테스트셋의 구분\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=4, activation='relu'))\n",
    "model.add(Dense(10,activation = 'relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 20)                100       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 343\n",
      "Trainable params: 343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 모델 저장 폴더 만들기\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 모델 저장 조건 설정\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.11227, saving model to ./model/01-1.1123.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.11227 to 0.95731, saving model to ./model/02-0.9573.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.95731 to 0.79245, saving model to ./model/03-0.7925.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.79245 to 0.70090, saving model to ./model/04-0.7009.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.70090 to 0.62610, saving model to ./model/05-0.6261.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62610 to 0.54831, saving model to ./model/06-0.5483.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.54831 to 0.47437, saving model to ./model/07-0.4744.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47437 to 0.45731, saving model to ./model/08-0.4573.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.45731 to 0.37114, saving model to ./model/09-0.3711.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.37114 to 0.36158, saving model to ./model/10-0.3616.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.36158 to 0.32462, saving model to ./model/11-0.3246.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.32462 to 0.29264, saving model to ./model/12-0.2926.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.29264 to 0.27448, saving model to ./model/13-0.2745.hdf5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.27448\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.27448 to 0.26748, saving model to ./model/15-0.2675.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.26748 to 0.22513, saving model to ./model/16-0.2251.hdf5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.22513\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.22513 to 0.19457, saving model to ./model/18-0.1946.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.19457 to 0.16520, saving model to ./model/19-0.1652.hdf5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.16520\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.16520 to 0.15264, saving model to ./model/21-0.1526.hdf5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15264\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.15264 to 0.13668, saving model to ./model/23-0.1367.hdf5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.13668\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.13668\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.13668 to 0.10757, saving model to ./model/26-0.1076.hdf5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.10757\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.10757 to 0.10524, saving model to ./model/36-0.1052.hdf5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10524\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10524 to 0.08247, saving model to ./model/38-0.0825.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08247 to 0.06069, saving model to ./model/39-0.0607.hdf5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.06069\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.06069\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.06069\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.06069\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.06069\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.06069 to 0.05287, saving model to ./model/45-0.0529.hdf5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.05287\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.05287 to 0.03800, saving model to ./model/56-0.0380.hdf5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.03800\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.03800 to 0.03038, saving model to ./model/68-0.0304.hdf5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.03038\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.03038 to 0.02605, saving model to ./model/100-0.0261.hdf5\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.02605\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.02605\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.02605\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.02605\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.02605\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.02605\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.02605\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.02605\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.02605 to 0.02326, saving model to ./model/109-0.0233.hdf5\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.02326\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.02326\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.02326 to 0.02173, saving model to ./model/112-0.0217.hdf5\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.02173\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.02173 to 0.01921, saving model to ./model/127-0.0192.hdf5\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.01921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00136: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.01921\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.01921 to 0.01700, saving model to ./model/157-0.0170.hdf5\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.01700\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.01700 to 0.01515, saving model to ./model/190-0.0151.hdf5\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.01515\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.01515\n"
     ]
    }
   ],
   "source": [
    "history =model.fit(X_train, Y_train,\n",
    "                   validation_split=0.2, \n",
    "                   epochs=500, \n",
    "                   batch_size=1,\n",
    "                   verbose=0,\n",
    "                   callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c3133926a0>]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2df5BdR5Xfv2d+yrCwjmURY2xFdvBSdsUpy57SMgXLzsZghFKFlDKbNVQyXtuxVpsoRJUQrV1gothVo11XsciAs4wM0nqohB9ZJcEsuMyieMrenQfWGBn8q+wVLD+EMRazmIQN+jGazh/92q9fT/e9fe+797137/t+ql69H/e+7tN9+557+vTpblFKgRBCSPUZ6rUAhBBCioEKnRBCagIVOiGE1AQqdEIIqQlU6IQQUhNGepXx+eefrzZs2NCr7AkhpJI8/vjjP1VKrfMd65lC37BhAxYXF3uVPSGEVBIR+X7oGF0uhBBSE6jQCSGkJlChE0JITUhV6CJyQEReEpGnAsdFRD4mIsdE5NsicnXxYhJCCEkjxkL/UwCbE46/C8Blzdd2AH/SuViEEEKykqrQlVKPAPjbhFO2AphTmq8DOFdEXl+UgIQQQuIoImzxDQB+aH0/3vztx+6JIrId2orH+vXrC8i62jQawPw8MDUFTE5mOy/2v3lkmpvTn6en0+WamwNefBG44ILw+UbWtWuBpaU4md3yuXIB7Xlv3AgcPdp+PK1+stRh2rn2cVs2G5+cSdfTTdNXhzFl8Mlm8jfp2nnkbVPudV67VpfV1z7s67lxY7hd+NrB3XcDzz0HrFsHXHFF+P+hOo1ps/v3A4cOAddfD2zf7i+rT36g/dr78ijr3gUAKKVSXwA2AHgqcOzLAN5qfT8M4Jq0NK+55ho1yCwsKHXOOUoND+v3hYX482L/m0emsTGlAP0aH0+Wyz43dL6RdWhInzM0lC6zW77Z2fa8Rkf1y87bfo2OalmS6idLHaadax8fG0uWzVdfadd4bEyf69bh7Gx6GZJks+vJ5JG3TbnXWSS5vG7b8bULXzsYGfHXpfv/UJ3GtNnZ2fZzZmdXl9Unf+ja23kUce8CWFQBvVqEhX4cwMXW94sAvFBAuqXis0R9T28g/DQ1T/GrrgLOPbfdenL/Z1svR48C3/wmcOoUsLICnD6tj/msl2eeAX75S/351Clg167WZ/Pfubk4q8Mtu23dLi0Bjz2m0zOcOgXs2aOtFLdc7rnAalkA4IUXgJMnddMGtMyhdM3/nniiVeaTJ4F77mnP68yZcNnMcRGd58mTOl3XQtqzp1WH5hxTNnONAF03n/50qwwnT2oL8YILWvVnyn72rE4vdouBU6eAW27Rspr0f/nLVvrmNzfNlRV93h/8QXs93X03sGlTu4V86FCrnGfPrq4nO01A52Pa2dVXA699ra6TNWvarWHX+n7xxfbr7KsDc90vvXT1NTTXwa7b559vr5cPfQhYXvbXpf3/TZuAH/ygdU1Mu/zmN1e3WXMN3vSm1r1w113t59x5py5r6D4x+bu/Gez7Ium+L4SQprdfSLbQ/ymABwEIgDcDeCwmzV5a6D5L1LUC0yw99ynuPqnt/xlLylgvaZaFK0foFbIKslrWIYvK/t0ul68cMZZzUrqx/8vz8llIbhnMtQ6VLenlWrpFlGV4ON//TL2677FliCl/bJpZ2myRL/ceLDK/PGX33RcxPdUQ6MRCF5HPApgCcL6IHAfwnwCMNh8GnwTwFQBbABwD8P8A3FTg86YU5ufbLYRTp/RT2bUa7O+u9bmw4E/bPHltC/Gee9qtF5s3vQn4zd/Un43lmGaB2v8V0Va8zalTwPvep3sOdk9jbg545BG/JeHKtm4d8NOftn5PskAuv1zL8vzzq2UJEZNuDOvWASdOJJ9z5oy2OC+8sN3qc8/Jy5kzwLZtLcvSx/PPA88+628DPlxrOhaTvv1u2mISb3xj/DWMLcO6dcBb3qI/m/KvrGgre2IC+MUv4tuLL+3LLwfOO2913Zo2ldQuY9qNj9iym7wBf/4TE8C+fT3yoZfx6icLPe9TtgyLIo+FWITseS3mUO+mjJdbjpi8O7EmO5Ut5A/uxGKMLU83e0BJMvjGQDqVyx17yHrPjI7Gtdmk+8a+Z91rMjoa9tmH2kcWULIPvXJMTmorfdcu7Q+z2bRJW3IvvAAsLuqnvYj2nx054k9vwwbg+9/Xl8oQYxUZVlZaFqJSwNCQluFHP4pPAwBe/Wrg7/5u9e8h6/Pyy3XvwPaLLi1p/+N996XnJwLcdJP+j5vH5Zfr47bldNFFul6NvzYk07p12opeswZ49FH9fxHt65yeXj32ceWVrd9efBH44hdb9XjppcB3vhOuxyzXycXuxSwvt6fj+kcnJ4HDh8ORMBdcoK24Rx7xyycCbN3a8pEfParP99WTfZ4Zo5ibA2Zn28+54ALt0z1yJL0ONmwANm9ur1+3DkSA3/gNfY1NnZ8+rf34do9DpOXTv/VW/ZsbrWTXi/2bPUa0d69O39yjsdfxllt05IppN768gFbPdteuVh3Z9WuPKb38sn6/8EJg9+6WfKF7b3m5BP85AK+W78arFxb6woJSMzP66Wze7Sfo8HBrRNu1qLZt8z+tR0f1Mdu6FtH/Gx31+zRHRlafPzSkX+b47t2roxx8EQ/2E3/37rCl5FoRxkKYnVXquuv0f2dm/JaPzyoU0bL4LJFQ9IY7lhCSyb5eWSMC3P/s3h32R4uE/ZtJde5GmQwN6fPsfPJYYG49xkTr+Nrqjh3tbdy8++oyKWLDzXthQaftGx9KisKyzxkZ0e+hqBbTBmPrKxQJlGbdh9Kz87f1hVu/sW0xz/VMAwkWuqi85kmHTExMqG4un9toANde2xphHhoCxse1H+vBB4EvfUlX+/i4tqTsqJeDB/UTdXgY2LKlPd0HH2wdu/lmHRXw0Y9q62NoSKdp8hPRv4+NAR/7mLaeXn4Z+MhHVlswa9Zo2XxRM+azHY1hrJb9+3VUxpo1+ve//MuWRTw0BLz1rTpaYXoaePJJ4Pd+b3W+hw+35+FGNXz5y7rMY2PAww/748RD8dWhWGX3f/Z1yxqza+exa5e+5iLan3veefocU4bRUeDjH2+Pakmrczvmef9+YOdOff1GRnT7iIk0SpI9FCceE7l08KC2Cm2r1W7roXjtUEy1Xf5rr9UWsWnrdnRY2jwJV76hIeDee1sx3ub+PH1atytzD8bUV9J9EbLufenY+e/bp9uO/f3o0ZYuiJUx7/VMQkQeV0pNeA+GNH3Zr25a6AsLSm3atNoiHB7WT+CZmXbra2am9d+ZmZblJaKfzr5jdlr2+bZlbj6bc00aPgvYPicvbtoi7Wled132fH1l7ldCshZZhqS0slqcnWLLUnR7KqLOktLodbty87/uuuR7u5dtHwkWeu1XW2w09NPwscfafWxDQ/opOzWln+jGil1Z0d8NU1PaIgH0/w8e1GmaY2Nj+rhJy/5tdNT/2Zxr0hgdbZfZlq0T3LTdNK+/Pnu+vjL3KyFZiyxDKC1j8d1xh343baZMjCxDzbtaRL8X0Z6KqLOkNHrdrtz8r78++d7u17Zf60HRUBigGQw03a/5ed3ojWtkaal17uSk7l6aASV7MMMd6DJdKHfwK2lqtck/Zhp0Vty03e6m6e7ak6PS8g2VuR8JyVpkGUJpzc+3T2wpZQAsQRbbpVVEeyqizpLS6HW78uV/5ZXJ93Y/tv3a+tBdn7mN8SnavvIk/11e/x4ZXNhmSFkk+dBrZ6GbQRIz9ddY3RPN4ptQRNtqSrMOem09kOrBNkN6Qa0sdNsqGh7WPkR7RBqg1UQIqTb1stAT4thsvyWgJy2sX5/PB1bqEpeEEFIC1VLoKY5JMwptDvtiTo2LpYNsCCGkL6mWQk8JHXBH+efn9eSZrKP8vYhQIISQTqmWQndNcE8gqFG8vlmhsZZ2RDaEENJ3VEuhR4YOzM+3hytmXUyeEQqEkCpSLYUORDnB7ZmfQL6ZcjG+dkII6SdqOfX/6NHWtGcR4O1v58AmIaT+1E6hNxrAgQOtdVvGxvT0fypzQkjdqZ1Cn59vxaGbDRiozAkhg0DtFLq9ItqaNa01iAkhpO5Ub1A0BUaoEEIGldopdIARKoSQwaR2LhdCCBlUqNAJIaQmUKETQkhNoEInhJCaQIVOCCE1gQqdEEJqAhU6IYTUBCp0QgipCVTohBBSE6jQCSGkJlChE0JITYhS6CKyWUSeE5FjInKb5/h6EXlYRI6KyLdFZEvxohJCCEkiVaGLyDCAewG8C8AVAN4rIlc4p30IwBeUUhsB3ADgvxQtKCGEkGRiLPRNAI4ppb6rlDoN4HMAtjrnKACvbX7+VQAvFCdiOo0GsHevfieEkEElZvncNwD4ofX9OIBfd87ZA+CrIvJvAbwawNt9CYnIdgDbAWD9+vVZZfXSaADXXgucPq03tuDeoYSQQSXGQhfPb8r5/l4Af6qUugjAFgCfEZFVaSul9iulJpRSE+vWrcsurcEyyefntTI/e1a/z8/nT5YQQqpMjIV+HMDF1veLsNqlcguAzQCglGqIyBoA5wN4qQgh23BM8ql938DY2JU4dUrvIbp2beE5EkJIJYix0I8AuExELhGRMehBzwecc34A4FoAEJHLAawBcKJIQV/BMcknl/4c+/bpPURXVoBdu+hLJ4QMJqkKXSm1DGAngIcAPAsdzfK0iNwpIu9unvYfANwqIt8C8FkAv6uUct0yxWDvAj02BkxNYWlJK/OVFeDkSWBurpScCSGkr5Gy9G4aExMTanFxMd+fG422XaAbDf3x9Gl9eHwcePhhDo4SQuqHiDyulJrwHavmTNHJSeD2219R5vPzwJYt2ocOAMvLHBwlhAweMYOifYs9Pmo8MMvLr3hiCCFkoKi0QrfHRwHg1luB9etf8cQQQshAUWmFbsZHzaSi6WkqckLI4FJphQ4AN96o36nMCSGDTmUVujvlf3q61xIRQkhvqWSUS6OhJxCdPMkp/4QQYqiche7GnAPAyAijWgghpHIW+vw8cOZM+2833UT/OSGEVE6hT00Bo6Ot7+Pj9J8TQghQQZfL5KS20s16LYxuIYQQTeUUOqAVOJU4IYS0UzmXCyGEED9U6IQQUhOo0AkhpCZQoRNCSE2gQieEkJpQXYXeaAB793IDUUIIaVLJsMVVK3MdPsw4RkLIwFNNC93e2YK7QhNCCICqKvSpKb3nHAAoBRw8SNcLIWTgqaZCn5wEbr6Zu0ITQohFNRU6oBdxWbOmtTs0188lhAw41RwUBbSVfviwtsy5KzQhhFRYoQNcpYsQQiyq63IhhBDSBhU6IYTUBCp0QgipCVTohBBSE6jQCSGkJlChE0JITYhS6CKyWUSeE5FjInJb4Jx/LiLPiMjTIvLfihWTEEJIGqlx6CIyDOBeAO8AcBzAERF5QCn1jHXOZQBuB/AWpdTPROR1ZQlMCCHET4yFvgnAMaXUd5VSpwF8DsBW55xbAdyrlPoZACilXipWTEIIIWnEKPQ3APih9f148zebXwPwayLyVyLydRHZ7EtIRLaLyKKILJ44cSKfxIQQQrzEKHTx/Kac7yMALgMwBeC9AD4lIueu+pNS+5VSE0qpiXXr1mWVlRBCSAIxCv04gIut7xcBeMFzzheVUmeUUn8D4DloBU8IIaRLxCj0IwAuE5FLRGQMwA0AHnDO+V8AfgsAROR8aBfMd4sUlBBCSDKpCl0ptQxgJ4CHADwL4AtKqadF5E4ReXfztIcALInIMwAeBvAflVJLZQlNCCFkNaKU6w7vDhMTE2pxcbEneRNCSFURkceVUhO+Y5wpSgghNaE+Cr3RAPbu5WbRhJCBpdo7FhkaDeDaa4HTp/X+oocPcycjQsjAUQ8LfW4OOHkSOHtWK/X5+V5LRAghXaf6Cr3RAA4cAMzg7siI3jSaEEIGjOor9Pl5bZkDgAhw0010txBCBpLqK/SpKe03Hx4G1qwBpqd7LREhhPSEegyK3nijfp+epnVOCBlYqq3Q3egWWueEkAGm2i6X+XmtzBndQgghFVfotv98bIzRLYSQgabaLpfJST2JaH5eK3P6zwkhA0y1FTqglTgVOSGEVNzlQggh5BWo0AkhpCZQoRNCSE2gQieEkJpAhU4IITWBCp0QQmoCFTohhNQEKnRCCKkJVOiEEFITqNAJIaQmUKETQkhNoEInhJCaQIVOCCE1gQqdEEJqAhU6IYTUhPop9EYD2LtXvxNCyABR/Q0ubNxNow8f5uYXhJCBoV4W+twccPIkN40mhAwk9VHojQZw4ACglP4+MsJNowkhA0WUQheRzSLynIgcE5HbEs57j4goEZkoTsR2gi7y+XltmWtBgJtuoruFEDJQpPrQRWQYwL0A3gHgOIAjIvKAUuoZ57zXAHg/gG+UISiQ4iKfmtI/moPT02WJUS6Nhn44TU3xgUQIyUTMoOgmAMeUUt8FABH5HICtAJ5xzrsLwN0APlCohBbz81pf2y7yV3Te5KTW8FVWhhzUJYR0QIzL5Q0Afmh9P9787RVEZCOAi5VSf56UkIhsF5FFEVk8ceJEZmGNET48rN9XucgnJ4Hbb6+uEvQ9sQghJJIYC108v6lXDooMAfgogN9NS0gptR/AfgCYmJhQKaevog5GeCKu24iDuoSQDMQo9OMALra+XwTgBev7awD8IwDzIgIAFwB4QETerZRaLEpQw+RkDRW5ofZPLEJImcQo9CMALhORSwD8CMANAN5nDiqlfg7gfPNdROYBfKAMZT4Q1PqJRQgpk1QfulJqGcBOAA8BeBbAF5RST4vInSLy7rIFJIQQEkfU1H+l1FcAfMX57cOBc6c6F4sQQkhW6jNTlBBCBhwqdEIIqQn1VehcRpcQMmDUa/lcA2dcEkIGkHpa6JxxSQgZQOqp0FPXCCCEkPpRT5cLZ1wSQgaQelroXIKWEDKA1M9C54AoIWRAqZ+FzgFRQsiAUj+FzgFRQsiAUj+XixkQnZvrtSSEENJV6mehG+6/H9i/H3jb2/Q7IYTUnHoq9Pl54NQpYGUFWF4Gdu7kEgCEDCoDtAxI/VwugPabDw1phQ7oAdK2HaUHCIZwkkFmwKLe6mmhT04C994LjI5qxT4+PpiDo6Yx33GHfh8AC4UMIEkW+IBFvdXTQgeA7duBK68cbOvU15gHsR5IfUmzwAds4/X6KnSA+3MOWGMmFSePezDNaBmwZUDqrdAHnQFrzKTC5PV1xxgtA2TYUaHXnQFqzIlwcLi/yesepNHSRv0VOm9kMmCRDqVR5r3UiXuQRssr1Fuh80YmQLv1d/KknkU8OcmHfRbKvpd6ZWnXrA3UW6EzyoMA+mYdHtbtQCng4EFg40Zg1y4+7GPpxr3UbUu7hgZfPePQDWUv1DVAM9AqzeQkcPPNgIj+vrwMHDo0UPHJHdPNRe9891XMvZb1fqxhjHq9LfQyF+qq4dO91kxP6/V9zPW6/nrg0UcZ0hlLt1wivvsKSL/X8tyPNQzrrbdCN5gb+f77i1O83XTnpPn5svgBa+YzjMankPJOPOuHOuyFDN1wiYSs5rR7Lc/9WMcIGaVUT17XXHON6gozM0oNDysF6PeZmWLSXVhQ6pxzdJrnnKO/l0FaPlnk6JbMdaYf6rAfZCgLX9liylvnOnEAsKgCerXePnTA7/srwvdtnu533VWuuyXNzzc3pyM3zp7VK0zu2RMuVw19hl2nH+owSYaqj+v47quYe63M+7FKdRrS9GW/umahK6Wf1jMz8U/7svLO+3+fxTIzo9TsrFJjY7r3YV5DQ7RiyqQf6jAkQ6eyddpWu51uN+iH6+2ABAt9MHzotu9v797u+r5DAzWxPlDXzwe00hRpLREMtL6HytWJz7Af/Mb9QK/9ruY67NsHLC21y9DJuE7eQf6Y8R3fIGdV2lLVQp9Dmr7sV1ctdJtuPnFD/vtOZLDTHBpSanRUfx8bU2p8PF+aaRZUGXVWZautV+QZT4mt5zxjTTHtwk13x45y7r8yexdZ5O1Cu0anFrqIbAZwD4BhAJ9SSv2hc/zfA/hXAJYBnABws1Lq+8U+egqimxZWKCyqk6e+m6ZtqZm0s0TDxFhmMfJmicQBqhfy2c0eSiivLCsLrl2rx1cOHtRx92n1nCeEz5bHjN/s2ZO8fC1QvMVbZghxFn3RD6HMIU1vXtBK/DsALgUwBuBbAK5wzvktAK9qfv59AJ9PS7dnFnq38T2xe+HrDOW5Y4dSIsmWWaeRNub40JBSIyNKbdtWTuRRWST5rIu2xpLq0j02O+vP35xnrmtsPSeVJ6kdDw0lj9+kjWF1Wo9lRbK5pMnZJTmQYKHHKPRJAA9Z328HcHvC+RsB/FVaul1X6PZAYugm6KYLoNv5+RrbwkL7oOr4eLLbJSRvWkOemWnd9OacvO6hXhCquzJcB2l1abfjUP52GoBW7J3ImPaQue661vXN+uAooh7T5CviPuuj0MlOFfp7oN0s5vu/BPCJhPM/AeBDael2PcolyZKItXzKlK/s/HyNzb7xRbS13mnaY2M6HfemGhlpKZihIX1OUWUuu/7S6q4X8xuS8k+7Hlnx+cHt+i5qTKiTeiyjJxySM+le6bEPPUah/7ZHoX88cO6/APB1AOOB49sBLAJYXL9+fWkFXoVrsbiNJzTQ2K2wxm4N0rqNrci8FxZ0Iw9Z3rOzul6Twip9Msbk260JXml11+nNnNaLdM9Nc3MV+cC0HxC+a5w3v6Kvny1H3oeFryyzs+06JKk3WzJdcbkAeDuAZwG8Li1N1c8W+shItu5jp2RpdGX5bItKM9ZdkKTMs97c3fKf+ijSdZDn/2Vag74H2MyMfmgXXd/GGOi0J+Hraeep09BM1axjEiWRpNBjolyOALhMRC4B8CMANwB4n32CiGwEMAtgs1LqpYg0u4u9SNeLLwIXXKAXazIj0G50gL2satkL9sRGF+zfD+zcqaMDxseLG0Evcn2OtLKk5ZUn+qdXCyy5kSh55jfYaeRdi6SMKIpQtIZZQ95e5Kyo+s6y3lJsFNDSUvaItqS1ZLThqud79OtiXiFNb78AbAHwPHS0ywebv90J4N3Nz18D8BMATzRfD6Sl2ZNB0dindb8NkPp80P0aGdJJ3eW1cntxvZLWG4nxWxdhTZZBzCBn0fWdtZcaGwWUZ3C0qGtbYntEJy6Xsl5dV+hlds3LVihulMjoaFzDrOLEnX57mPpImjCWNI6Qlkavr1maa7LsfGMeZmlhtkUMjobSsN1rSeGdJT+YqdCVKmcQK5Ru0dg32siItuaKlqnXyqQXdNIjiAkZTDIcutFu7LyyzhYdGtKWep6xgDz+8BgZFxbyhdnaBlGnvdu069aFMR0qdEPMIFZWxdYvkxpCMsWEI3ZTufQTnVy70PXoN9deFjdEpy4vd7G44eF24yNLeX3nZgkdtMuxe3dLJiDdIEoiZuCfFnoPKGqySGw4XlY69UXHWjJKpccZlyVnr0nq+XRa/74IkSLSykro2oZ89lllt++ZkZH2SBDbPZj1QReSLU+Mvj0mULaFbs6hD73L+C5M7DR418qPdYV0IltWYsriyy/LIl9FyNlp4+/0/74HcpFWVidpFVW/vmsbE54bk7/rpnHnexgFmqXXmDZpKmuMftEDzj02YpIU+mAsn+vD3W/0ySeBAwdaoUkjI6vDktxwrne+U28uoZQOZVpaWn2+CYV0lzp16TSEzU0L0DKahZmSQqzssM0f/AC47764vNPkzLO0apZFv4pYDGlpSS85bC87DBS3gFQn17KIpVtD13ZoSG/6khSCF5O/b7G4Bx8EvvQlfV+Mj7fSHh7WaSmlFw2zQ4dDaQ4Pa7kbjVboZEwd3Hijfjd5FLndoL1csf29Hwhp+rJfPbfQlVrdXTQWi2tBGOt906b2aBP7s+vWsK33rBtPdGJR2BZnKMQqycLI0qVMkjOrdZfHH1nE+IVrwe7Y0SpXET0vtwcwOxs/aFi0P9YnSxETvXztyfdb1l5jbMRQSOZOlz3I4vqJrYMCAF0uHhYW2n1rIvqzbxapuyuQ+/J1IW1lkza7rKgQtoWF9Hj1Tn2AvoeP79wYZdupwi5q/MKnPHbv7jxt1yW3e3e2sQ2Thnl4xrga0q5b1odU1nbYqbFgk+eB7d53oYXJYsoVyt83LpEUu15kVJ2iQl+Naz3bL7exz8ysHuhxX74bM4+F3qlFGBOv3qlVG2tldWLdxaThU5ad3Cxuua67LlukkA/f4JzdlkSS6y8tIss9vxvhdK5cWaNksii1PD0U85+kafpZ2maMhb5t2+p7IqT0C+j1UaG7uAM09sV3rVrXQjcLdyVZ5/Z/Y62rIqzNmAdDJ914d4Gi0dHkLm0RFkkoDXcwrpMF1dxrPD6+OgQvz2JMvt5MTJru/2LWT+lGOJ3rznDdIZ1ESyXlmeYW8rk6ktw1WR5uofzte9t3TX3XMOvkwABU6C52ZbuhVr6KNg3EKK+8yjepceaxoPL67WIVrWuN2e4cI2eJ8bapsvnGP2InxNhlC0VgZPH5xuRjvqf50GO69En1kXTOjh3aoszjWw4ZQrabME+0VF7SypykjEOWd9YHUFL0jq307Ydyh+GTVOg+7Mrulm8xrfH5fG6hm78IiyuLvK6FYcYbsiq7gvyIbWnZ1zDNveUrW1JMdpl17CtLyMVSxIPa1xPJosxiFLZJx1ZgWVxWWYyUTtxIvgdtnmud1it2DY/h4Y7HfKjQ00jrVsU0+rQbInaQ0L6pk7rnWRtzVkWa5gPcvTu7j9ynSGPrN42Fhfidc7IMQrsP1SIfSHYeeQfQssjjjgcZH34WZeZez6T7Ic8AcMzgYtL5eenk4WB67CL+MbiC3VBU6HmIteJ852bpEocskrQBtKw3Yd7BpSQlE2MRhnzBrt+7iMkfsYoxb93FbO6QR+HnVSZZr2tIySbl38kDLOtEPZ8c7liJ604r6gHbycMhKRihhF5ekkIf3IlFabiTKg4dCk+yiJmA4ds93DcxBtC/nTqlm4fBnfyRZTfy0BrPSf8Npe/uMp8lX1OO06f1hJazZ1sTepLqNwYzAWTfvtYkLiC8rneeultZaV0Tux5NHmaijpnMFR5E5Z8AAA0ZSURBVDvRae1aPdFnZUX/f+3auDJnndg1Oam/m8l09sQe35rynU7cmp5OXjvdTX/fPr8cY2P6flhZAb72NeDRR9uvZacThYBsbcJlaqp1/QB9Pcy1cCcwlk1I05f9qoyFHuNiyPsUjrFINm3q/U4uPss8ZnJMksXsytCJhR6q/yJC9ex24AtVTRsojE3f/M+Om87aA8rSJmN6W0W49ZLKEOv6yuJOS5OvrDGRpECJgvMFXS45yTKzLk/XL6vPsBNCXdu0qBDfwyCLbzTNz1rEGEVI8RRVl65CsQf5YlwySWWw3RJ29FBMZEtSvaS5UWLT7rZbr8j0XYp4wLsyxbTfgvNNUuh0uYRoNLQbwHYLLC0Bt9/uP992vzz5ZPraLeY/vm5e3q5fEm7X1NeNtd0VIXfSoUPAmTOtdNLcI0ldYvdY6Ny0rn9oG7qiuruTk8CePbqOTB7T063j9rohQOvaAXprupD7B2hfPwjQXfexMf055E6JWSMoaWu+2DViOnXrpa3rk5a++59O74sitytM2qavzHzTCGn6sl99baG73eyYMKOkrrN9TtEREnlxrc7Q5JwsFnqR5Qv1KEIhcKG8Q5Zd3h5VbKibe8w3Ocgt17ZtyaGLdrqxM5Cz1Ekn5HH/pPXIyu6pdkLZUWYJgC6XjGRxSdj/cf2soa550f67vNgyucup2utmu/7yhQX/b6GQxKyNOe0hMjwcP1/AFzaWZ9GnmLTtm9o9tm2bf5GurC4aO127PrJGppRhXGRxOaRNzisr8iZvGXznZRmjKBAq9Kzk9Qe6MylDg2fmIRGzJEBR5UnzTduDksYXnKWH4pbPWPsxMwZd+Xzhbjt2tNfr0FBYqdvp+XzcSet8ZCHWKrXr0x1gz7oiYJqFXmSPpCh8D+i8i8gVYRiFfN+dBgukyZ72v0io0POQp9KN1eGzntwb0SiVsjfizXIDmDK705TTFJ+x2H2bJ6RFfsS4dHxTp91436TyhsoVWokva/2mPSxdd4u98FeeB4qtgGIiU7IqwbKt96S47TQ5YtxvaXKEoqtiNv2IoYgB6QSo0LuJq9xc62LTJn9UQ6cj7iHy+PqM/FnXcnfXE4+x0F353GiSbdvab7i0hdSSwtt8snbDYk1SIjHjLFkUrE9hZGkD3XAN2sZN1pUyFxY6WzTN1958PctOo6JC7ayAiJckhc4ol6IxI93T0/4R+See8Ec1ZB35diMAQhMmsoyw2yP3w8PA9u3Axo3JETt2dAMArF+vd4cJRX6kRWNcdRXw1a/qY6aeTPrDw8DWrcCXv6y/27vhGNlN5I6vXmMjJdJ2Wsr6H1++vh10kiaapU3usfP3lTG2DRSxS1Iadn2sXQvs2hU/eWlyErj5ZmB2VreP5eVsMrq7Ib3qVXp3MqA1uSkmQi2mfHNzemem++7TE6wOHy4/4iWk6ct+1dZCTyIpqiELWSw+c35MPnmshxhZ0vIPdceHhsKbByR1xWMHsmPKk9cqdssVg6/+Y65JjMxFDfYVTRFtLs/9Y/ei8+zsFUPS/IiSfOi00LtFo6H3RrStgd27s1sBjYaOiTaWaMy0+djp0XmsB9f6dC28ubn26d8+C8yOeV+7Vlvedrx3qLdj7+noyr5nTz4LK4+FGlpaIeu0+VD9p12T2KUnYnskZfVifHTS5vLOLzBLICwvt/cs3bj5TverTZofUdI+pFTo3cB1Zdx6a3iDXN9/7YkqPrfC9de3Jr24m+pmIe/kDd+kJdOIgfR1ZHxreiwtaeVuzrMndIVutiImZJl1VZSKVzC+GzfPgyHvRLNOuvFJE2QaDT0xyp3AVISySytzDMZQMO6MrOuvJNVZEa6notpkFkKme9mvgXK55B0IcbuW7mqFtlshaTC224TCBkMumayRGUVP4bbltgfr7LDILG4jpYrb7zRW7tiNp12S3AKh8Miy6r8IubPgttNeXb+MgC6XHpPXgnKtBCDsVnC7kTFWRVHdZhfXYk9yyZjfs1i4MfWZVLbQMZOnWfVwaal1fppFape50dADfWfPamt/375i6tcntyubvSRBDKG6tOsCaLn3Qter2+SRwbf6ZKiH+P736yUuhoeTr19Z91BeQpq+7NdAWehK5RsISYqpTgshjIk578XM1dgBxDT5YuohZGlnnaKe1Rosw4ItSrZQ2r5QybQJTL2aqGTL2Ek4p8Gtw02bVFtYcdKewT24h0ALvQ/IMxAS8sElLXZV1GJKZZFUJncgN6ksSfU5P98aZ1hZAXbu1GGCphcTKncoz6zWYBkWbEjuIvLy1aUbWuiG8pU4sBdNFhmy9PguvLDzNHtFSNOX/Ro4C73f6JWF3g0WFsJTy/OWO6tFWrQFm9azyJtXP1ja3SBLj89MXhLR70njJn1moYs+3n0mJibU4uJiT/ImTfrN/1ck+/dry9xMQLL93lUtd9FyFxmtUgWy1F/suT1oSyLyuFJqwnssRqGLyGYA9wAYBvAppdQfOsfHAcwBuAbAEoDfUUp9LylNKnRSOlVV3N1i717gjjtas3Dvuiu83j/pG5IUeqoPXUSGAdwL4B0AjgM4IiIPKKWesU67BcDPlFJvFJEbAPwRgN/pXHRCOqAf/Lz9TD9Eq5BCiRkU3QTgmFLquwAgIp8DsBWArdC3AtjT/PxnAD4hIqJ65c8hhKTTi4kvpFRiFPobAPzQ+n4cwK+HzlFKLYvIzwGsBfBT+yQR2Q5gOwCsX78+p8iEkMJgL6ZWDEWcI57fXMs75hwopfYrpSaUUhPr1q2LkY8QQkgkMQr9OICLre8XAXghdI6IjAD4VQB/W4SAhBBC4ohR6EcAXCYil4jIGIAbADzgnPMAgOYC2HgPgP9N/zkhhHSXVB960ye+E8BD0GGLB5RST4vIndAB7g8A+DSAz4jIMWjL/IYyhSaEELKaqKn/SqmvAPiK89uHrc8nAfx2saIRQgjJQozLhRBCSAXo2dR/ETkB4Ps5/34+nJDIClFV2Sl3d6mq3EB1Za+K3P9AKeUNE+yZQu8EEVkMTX3td6oqO+XuLlWVG6iu7FWV24YuF0IIqQlU6IQQUhOqqtD391qADqiq7JS7u1RVbqC6sldV7leopA+dEELIaqpqoRNCCHGgQieEkJpQOYUuIptF5DkROSYit/VaniRE5Hsi8qSIPCEii83fzhORvxCRv26+/70+kPOAiLwkIk9Zv3nlFM3HmvX/bRG5uneSB2XfIyI/atb7EyKyxTp2e1P250Tknb2RGhCRi0XkYRF5VkSeFpF/1/y9r+s9Qe6+rnMRWSMij4nIt5py/+fm75eIyDea9f355npVEJHx5vdjzeMbeiF3ZkKbjfbjC3otme8AuBTAGIBvAbii13IlyPs9AOc7v90N4Lbm59sA/FEfyPk2AFcDeCpNTgBbADwIvWTymwF8ow9l3wPgA55zr2i2mXEAlzTb0nCP5H49gKubn18D4PmmfH1d7wly93WdN+vtV5qfRwF8o1mPXwBwQ/P3TwL4/ebnfw3gk83PNwD4fC/qO+urahb6K7snKaVOAzC7J1WJrQDub36+H8C2HsoCAFBKPYLVyx2H5NwKYE5pvg7gXBF5fXckXU1A9hBbAXxOKXVKKfU3AI5Bt6muo5T6sVLqm83P/xfAs9AbxfR1vSfIHaIv6rxZb79ofh1tvhSAfwK9yxqwur7NdfgzANeKiG/fh76iagrdt3tSUmPqNQrAV0Xk8eZuTQDw95VSPwb0zQHgdT2TLpmQnFW5BjubrokDllurL2Vvduc3QluNlal3R26gz+tcRIZF5AkALwH4C+jewstKqWWPbG27sAEwu7D1NVVT6FE7I/URb1FKXQ3gXQD+jYi8rdcCFUAVrsGfAPiHAK4C8GMAH2n+3neyi8ivADgEYJdS6v8kner5rWeye+Tu+zpXSp1VSl0FvUnPJgCX+05rvveN3FmomkKP2T2pb1BKvdB8fwnA/4RuRD8xXeXm+0u9kzCRkJx9fw2UUj9p3rwrAO5Dq4vfV7KLyCi0UvyvSqn/0fy57+vdJ3dV6hwAlFIvA5iH9qGfK3qXNaBdtkruwlY1hR6ze1JfICKvFpHXmM8ArgPwFNp3d7oRwBd7I2EqITkfADDdjLp4M4CfGxdBv+D4lv8ZdL0DWvYbmhEMlwC4DMBj3ZYP0FEr0BvDPKuU+mPrUF/Xe0jufq9zEVknIuc2P58D4O3Q/v+HoXdZA1bXd/V2Yev1qGzWF/Ro//PQ/q8P9lqeBDkvhR7d/xaAp42s0H64wwD+uvl+Xh/I+lnobvIZaMvklpCc0F3Re5v1/ySAiT6U/TNN2b4NfWO+3jr/g03ZnwPwrh7K/VboLvy3ATzRfG3p93pPkLuv6xzAPwZwtCnfUwA+3Pz9UugHzDEA/x3AePP3Nc3vx5rHL+1lO499ceo/IYTUhKq5XAghhASgQieEkJpAhU4IITWBCp0QQmoCFTohhNQEKnRCCKkJVOiEEFIT/j/YC8rA5paJJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_vloss=history.history['val_loss']\n",
    "y_acc=history.history['accuracy']\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.0476 - accuracy: 0.9667\n",
      "\n",
      " Accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model  \n",
    "model = load_model('./model/230-0.0159.hdf5')\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 131\n",
      "Trainable params: 131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 모델 저장 폴더 만들기\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.08442, saving model to ./model/01-1.0844.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.08442 to 0.97179, saving model to ./model/02-0.9718.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.97179 to 0.90487, saving model to ./model/03-0.9049.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.90487 to 0.85003, saving model to ./model/04-0.8500.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85003 to 0.75067, saving model to ./model/05-0.7507.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.75067 to 0.74130, saving model to ./model/06-0.7413.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.74130 to 0.65011, saving model to ./model/07-0.6501.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.65011 to 0.64076, saving model to ./model/08-0.6408.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.64076 to 0.56126, saving model to ./model/09-0.5613.hdf5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.56126\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.56126\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.56126 to 0.51539, saving model to ./model/12-0.5154.hdf5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51539\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.51539 to 0.50247, saving model to ./model/14-0.5025.hdf5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.50247\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.50247 to 0.47674, saving model to ./model/16-0.4767.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.47674 to 0.45409, saving model to ./model/17-0.4541.hdf5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.45409\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.45409\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.45409 to 0.44041, saving model to ./model/20-0.4404.hdf5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.44041\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.44041 to 0.41447, saving model to ./model/22-0.4145.hdf5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41447\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.41447 to 0.39860, saving model to ./model/24-0.3986.hdf5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.39860\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.39860\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.39860\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.39860\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.39860 to 0.35839, saving model to ./model/29-0.3584.hdf5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.35839\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.35839 to 0.32726, saving model to ./model/31-0.3273.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.32726 to 0.28343, saving model to ./model/32-0.2834.hdf5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.28343\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.28343\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.28343 to 0.27120, saving model to ./model/35-0.2712.hdf5\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.27120 to 0.25765, saving model to ./model/36-0.2577.hdf5\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.25765 to 0.23241, saving model to ./model/37-0.2324.hdf5\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.23241 to 0.22451, saving model to ./model/38-0.2245.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.22451 to 0.19767, saving model to ./model/39-0.1977.hdf5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.19767\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.19767\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.19767\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.19767 to 0.19369, saving model to ./model/43-0.1937.hdf5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.19369\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.19369 to 0.19363, saving model to ./model/45-0.1936.hdf5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.19363 to 0.16965, saving model to ./model/46-0.1696.hdf5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.16965\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.16965\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.16965\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.16965\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.16965 to 0.15040, saving model to ./model/51-0.1504.hdf5\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.15040\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.15040\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.15040\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.15040\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.15040 to 0.13317, saving model to ./model/56-0.1332.hdf5\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.13317\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.13317\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.13317\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.13317\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.13317\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.13317 to 0.12137, saving model to ./model/62-0.1214.hdf5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.12137\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.12137 to 0.11223, saving model to ./model/73-0.1122.hdf5\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.11223\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.11223\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.11223\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.11223 to 0.10198, saving model to ./model/77-0.1020.hdf5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.10198\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.10198\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.10198\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.10198 to 0.10020, saving model to ./model/81-0.1002.hdf5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.10020\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.10020\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.10020\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.10020\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.10020\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.10020 to 0.09598, saving model to ./model/87-0.0960.hdf5\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.09598\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.09598\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.09598\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.09598\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.09598\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.09598\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.09598\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.09598\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.09598 to 0.09530, saving model to ./model/96-0.0953.hdf5\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.09530\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.09530\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.09530\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.09530\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.09530\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.09530 to 0.07404, saving model to ./model/102-0.0740.hdf5\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.07404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00130: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.07404\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.07404 to 0.06536, saving model to ./model/138-0.0654.hdf5\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.06536\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.06536 to 0.06025, saving model to ./model/177-0.0603.hdf5\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.06025\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.06025\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.06025\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.06025\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.06025\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.06025 to 0.05727, saving model to ./model/183-0.0573.hdf5\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.05727\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.05727 to 0.05441, saving model to ./model/215-0.0544.hdf5\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.05441\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.05441 to 0.05422, saving model to ./model/263-0.0542.hdf5\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.05422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00285: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.05422\n",
      "\n",
      "Epoch 00330: val_loss improved from 0.05422 to 0.04568, saving model to ./model/330-0.0457.hdf5\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.04568\n",
      "\n",
      "Epoch 00343: val_loss improved from 0.04568 to 0.04038, saving model to ./model/343-0.0404.hdf5\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.04038\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.04038\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.04038\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.04038 to 0.03888, saving model to ./model/347-0.0389.hdf5\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.03888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00440: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.03888\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.03888\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장 조건 설정\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "history =model.fit(X_train, Y_train,\n",
    "                   validation_split=0.2, \n",
    "                   epochs=500, \n",
    "                   batch_size=1,\n",
    "                   verbose=0,\n",
    "                   callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c31499fa90>]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_vloss=history.history['val_loss']\n",
    "y_acc=history.history['accuracy']\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "\n",
      " Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./model/347-0.0389.hdf5')\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 모델 저장 폴더 만들기\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.83742, saving model to ./model/01-1.8374.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.83742 to 1.33186, saving model to ./model/02-1.3319.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.33186 to 1.06136, saving model to ./model/03-1.0614.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.06136 to 1.04127, saving model to ./model/04-1.0413.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.04127 to 0.99545, saving model to ./model/05-0.9954.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.99545 to 0.98343, saving model to ./model/06-0.9834.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.98343 to 0.91012, saving model to ./model/07-0.9101.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.91012 to 0.90772, saving model to ./model/08-0.9077.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.90772 to 0.85646, saving model to ./model/09-0.8565.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.85646 to 0.84099, saving model to ./model/10-0.8410.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.84099 to 0.81875, saving model to ./model/11-0.8188.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.81875 to 0.76672, saving model to ./model/12-0.7667.hdf5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.76672\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.76672 to 0.74089, saving model to ./model/14-0.7409.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.74089 to 0.70702, saving model to ./model/15-0.7070.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.70702 to 0.70453, saving model to ./model/16-0.7045.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.70453 to 0.67819, saving model to ./model/17-0.6782.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.67819 to 0.67600, saving model to ./model/18-0.6760.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.67600 to 0.64701, saving model to ./model/19-0.6470.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.64701 to 0.63475, saving model to ./model/20-0.6347.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.63475 to 0.62563, saving model to ./model/21-0.6256.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.62563 to 0.59905, saving model to ./model/22-0.5990.hdf5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.59905\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.59905 to 0.58739, saving model to ./model/24-0.5874.hdf5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.58739\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.58739 to 0.55985, saving model to ./model/26-0.5598.hdf5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.55985\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.55985 to 0.54410, saving model to ./model/28-0.5441.hdf5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.54410\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.54410\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.54410\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.54410\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.54410 to 0.53955, saving model to ./model/33-0.5396.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.53955 to 0.50070, saving model to ./model/34-0.5007.hdf5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.50070\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.50070\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.50070 to 0.48931, saving model to ./model/37-0.4893.hdf5\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.48931 to 0.47812, saving model to ./model/38-0.4781.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47812 to 0.46802, saving model to ./model/39-0.4680.hdf5\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.46802 to 0.44536, saving model to ./model/40-0.4454.hdf5\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.44536\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.44536 to 0.43866, saving model to ./model/42-0.4387.hdf5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.43866\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.43866 to 0.41950, saving model to ./model/44-0.4195.hdf5\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.41950\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.41950 to 0.41328, saving model to ./model/46-0.4133.hdf5\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.41328 to 0.41111, saving model to ./model/47-0.4111.hdf5\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.41111 to 0.39774, saving model to ./model/48-0.3977.hdf5\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.39774 to 0.38099, saving model to ./model/49-0.3810.hdf5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.38099\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.38099\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.38099\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.38099\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.38099\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.38099 to 0.37830, saving model to ./model/55-0.3783.hdf5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.37830\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.37830 to 0.35633, saving model to ./model/57-0.3563.hdf5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.35633\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.35633\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.35633 to 0.34936, saving model to ./model/60-0.3494.hdf5\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.34936 to 0.34443, saving model to ./model/61-0.3444.hdf5\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.34443 to 0.33417, saving model to ./model/62-0.3342.hdf5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.33417\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.33417\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.33417 to 0.31870, saving model to ./model/65-0.3187.hdf5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.31870\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.31870 to 0.29783, saving model to ./model/67-0.2978.hdf5\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.29783\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.29783\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.29783 to 0.28641, saving model to ./model/70-0.2864.hdf5\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.28641\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.28641\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.28641\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.28641\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.28641\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.28641 to 0.27964, saving model to ./model/76-0.2796.hdf5\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.27964\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.27964\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.27964\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.27964\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.27964\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.27964\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.27964 to 0.27902, saving model to ./model/83-0.2790.hdf5\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.27902 to 0.25164, saving model to ./model/84-0.2516.hdf5\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.25164\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.25164 to 0.24301, saving model to ./model/86-0.2430.hdf5\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.24301\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.24301\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.24301 to 0.22782, saving model to ./model/89-0.2278.hdf5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.22782\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.22782\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.22782\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.22782 to 0.20765, saving model to ./model/93-0.2076.hdf5\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.20765\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.20765\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.20765 to 0.19866, saving model to ./model/96-0.1987.hdf5\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.19866\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.19866\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.19866\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.19866\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.19866 to 0.19727, saving model to ./model/101-0.1973.hdf5\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.19727 to 0.19376, saving model to ./model/102-0.1938.hdf5\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.19376\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.19376\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.19376\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.19376 to 0.18508, saving model to ./model/106-0.1851.hdf5\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.18508\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.18508\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.18508\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.18508\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.18508\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.18508 to 0.17481, saving model to ./model/112-0.1748.hdf5\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.17481\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.17481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00115: val_loss did not improve from 0.17481\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.17481\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.17481\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.17481\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.17481 to 0.16178, saving model to ./model/119-0.1618.hdf5\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.16178\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.16178\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.16178 to 0.15853, saving model to ./model/122-0.1585.hdf5\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.15853\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.15853 to 0.15779, saving model to ./model/124-0.1578.hdf5\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.15779\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.15779\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.15779 to 0.14933, saving model to ./model/127-0.1493.hdf5\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.14933\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.14933\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.14933\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.14933\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.14933\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.14933\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.14933 to 0.14180, saving model to ./model/134-0.1418.hdf5\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.14180\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.14180\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.14180\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.14180\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.14180\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.14180 to 0.13987, saving model to ./model/140-0.1399.hdf5\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.13987\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.13987\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.13987 to 0.13535, saving model to ./model/143-0.1354.hdf5\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.13535\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.13535\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.13535\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.13535\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.13535\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.13535 to 0.13494, saving model to ./model/149-0.1349.hdf5\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.13494\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.13494\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.13494\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.13494\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.13494\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.13494 to 0.13102, saving model to ./model/155-0.1310.hdf5\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.13102\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.13102\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.13102\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.13102\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.13102 to 0.11186, saving model to ./model/160-0.1119.hdf5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.11186\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.11186 to 0.10363, saving model to ./model/175-0.1036.hdf5\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.10363\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.10363 to 0.09859, saving model to ./model/203-0.0986.hdf5\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.09859\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.09859\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.09859\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.09859\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.09859 to 0.09789, saving model to ./model/208-0.0979.hdf5\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.09789\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.09789 to 0.09521, saving model to ./model/239-0.0952.hdf5\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.09521\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.09521 to 0.09102, saving model to ./model/257-0.0910.hdf5\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.09102\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.09102\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.09102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00261: val_loss did not improve from 0.09102\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.09102\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.09102\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.09102\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.09102\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.09102\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.09102 to 0.08052, saving model to ./model/267-0.0805.hdf5\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.08052\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.08052 to 0.06974, saving model to ./model/304-0.0697.hdf5\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.06974\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.06974\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장 조건 설정\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "history =model.fit(X_train, Y_train,\n",
    "                   validation_split=0.2, \n",
    "                   epochs=500, \n",
    "                   batch_size=1,\n",
    "                   verbose=0,\n",
    "                   callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c3131b2240>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_vloss=history.history['val_loss']\n",
    "y_acc=history.history['accuracy']\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 993us/step - loss: 0.0339 - accuracy: 1.0000\n",
      "\n",
      " Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./model/304-0.0697.hdf5')\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_model = load_model('./model/304-0.0697.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 2.6, 1. , 0.2],\n",
       "       [7.7, 2.8, 6.7, 2. ]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test = np.array([[5,2.6,1,0.2],[7.7,2.8,6.7,2]])\n",
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = l_model.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = ['setosa','versicolor','virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_df = pd.DataFrame(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.999616e-01</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.196996e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.014796e-21</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>9.983423e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1             2\n",
       "0  9.999616e-01  0.000038  1.196996e-12\n",
       "1  1.014796e-21  0.001658  9.983423e-01"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_df['rs'] = Y_pred_df.apply(lambda x : [ 1 if val == np.max(x) else 0 for idx,val in enumerate(x)] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>rs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.999616e-01</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.196996e-12</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.014796e-21</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>9.983423e-01</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1             2         rs\n",
       "0  9.999616e-01  0.000038  1.196996e-12  [1, 0, 0]\n",
       "1  1.014796e-21  0.001658  9.983423e-01  [0, 0, 1]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_df['rs'] = Y_pred_df['rs'].apply(lambda x : tg[0] if str(x) == '[1, 0, 0]' else (tg[1] if str(x) == '[0, 1, 0]' else tg[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>rs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.999616e-01</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.196996e-12</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.014796e-21</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>9.983423e-01</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1             2         rs\n",
       "0  9.999616e-01  0.000038  1.196996e-12     setosa\n",
       "1  1.014796e-21  0.001658  9.983423e-01  virginica"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
